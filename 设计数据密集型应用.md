# Designing data-intensive application

> Martin Kleppmann

## chapter 1: reliable, scalable, and maintainable applications

- tools of data storage and processing are optimized for a variety of different use cases, and are no longer neatly fit into traditional categories.
- there are datastores that are also used as message queues --- Redis, and there are message queues with database-like durability guarantees --- Apache Kafka
- the boundaries between the categories are becoming blurred
- a single tool can no longer meet all of its data processing and storage needs, those different tools are stitched together using application code.
- application programming interface --- API
- for data intensive system, you are now not only an application developer, but also a data system designer
- 3 concerns that are important in most software systems:

  - reliability: the system should continue to work even in the face of adversity
  - scalability: as the system grows, there should be reasonable ways of dealing with that growth
  - maintainability: many different people should be able to work on it productively

- Reliability: continuing to work correctly, even when things go wrong, called fault-tolerant or resilient
- reliability only makes sense to talk about tolerating certain types of faults
- note that a fault is not the same as a failure: a fault is usually defined as one component of the system deviating from its spec, wheareas a failure is when the system as a whole stops providing the required service to the user
- hardware faults is common. hard disks are reported as having a mean time of failure (MTTF) of about 10 to 50 years, thus, on a storage cluster with 10,000 disks, we should expect on average one disk to die per day
- there is a move toward systems that can tolerate the loss of entire machines, by using software fault-tolerance techniques in preference of in addition to hardware redundancy
- hardware faults as being random and independent from each other
- another class of fault is systematic error within the system, the are correlated across nodes, tend to cause many more system failures than uncorrelated hardware faults
- a software bug crash when given a particular bad input
- a runaway process that uses up some shared resource
- a service that the system depends on that slows down
- cascading failures
- it is revealed that the software is making some kind of assumption about is environment if the system meets software errors
- human errors are common.
- humans are known to be unreliable, one study of large internet services found that configuration errors by operators were the leading cause of outages, whereas hardware faults played a role in only 10-25% of outages.
- how do we make our systems reliable, in spite of unreliable humans:
  - design systems in a way that minimizes opportunities for error
  - decouple the paces where people make the most mistakes from the places where they can cause failures.
  - test thoroughly at all levels, from unit tests to whole-system integration tests and manual tests
  - allow quick and easy recovery from human errors
  - set up detailed and clear monitoring
  - implement good management practices and training
- scalability: is the term we use to describe a system's ability to cope with increased load
- even if a system is working reliably today, that doesn't mean it will necessarily work reliably in the future
- discussing scalability means considering questions like:
  - if the system grows in a particular way, what are our options for coping with the growth
  - how can we add computing resources to handle the additional load
- scalability needs: first describe the current load on the system (describing load) ;then investigate what happens when the load increases (describing performance); finally, how do we maintain good performance when our load parameters increase by some amount (approaches for coping with load)
- load can be described with a few numbers which we call load parameters, the best choice of parameters depends on the architecture of your system
- latency is the duration that a request is waiting to be handled, response time is what the client sees (includes network delays, queueing delays, the actual time to process the request).
- for performances, usually it is better to use percentiles : the response time thresholds at which 95%, 99% or 99.9% of requests are faster than that particular threshold, denoted as p95, p99, p999
- percentiles are often used in service level objectives (SLOs) and service level agreements (SLAs)
- an architecture that is appropriate for one level of load is unlikely to cope with 10 times that load.
- scaling up, vertical scaling, moving to a more powerful machine
- scaling out, horizontal scaling, distributing the load across multiple smaller machines
- very intensive workloads often can't avoid scaling out
- good architectures usually involve a pragmatic mixture of approaches
- some systems are elastic, meaning that they can automatically add computing resources when they detect a load increase, whereas other systems are scaled manually
- an elastic system can be useful if load is highly unpredictable, but manually scaled systems are simpler and may have fewer operational surprises
- the architecture of systems that operate at large scale if usually highly specific to the application -- there is no such thing as generi, one-size-fits-all scalable architecture
- an architecture that scales well for a particular application is built around assumptions of which operations will be common and which will be rare -- the load parameters an early-stage startup or an unproven product it's usually more important to be able to interate quickly on prodct features than it is to scale to some hypothetical future load.

## Chapter 2: Data Models and Query Languages

- NoSQL -- Not only SQL
- object-oriented programming languages, which leads to a common criticism of the SQL data model: if data is stored in relational tables, an award translation layer is required between the objects in the application code and the database model of tables, rows and columns.  -- impedance mismatch.
- Object-relational mapping (ORM) frameworks like ActiveRecord and Hibernate reduce the amount of boilerplate code required for this translation layer, but they can't completely hide the differences between the two models.
- The JSON representation has better locality than the multi-table schema
- in JSON representation, all the relevant information is in one place, and one query is sufficient.
- JSON is one-to-many relationship
- removing the duplication of redundant copies in the database is the key idea behind normalization in databases
- normalization requires many-to-one relationships, which don't fit nicely into the document model.
- in relation databases, it's normal to refer to rows in other tables by ID, BECAUSE JOINS ARE EASY. in document database, the support for joins is often weak.
- early in the development of database, the two most prominent were the relational model and the network model.
- network model also called CODASYL model, it was a generalization of the hierarchical(tree) model.
- relational model: a relation (table) is simply a collection of tuples (rows), and that's it. There is no complicated access path to follow if you want to look at the data.
- query optimizers for relational databases are complicated beasts.
- today, relational versus document databases
- if the data in your application has a document-like structure, then it's probably a good idea to use a document model.
- the poor support for joins in document databases may or may not be a problem, depending on the application.
- if your application does use many-to-many relationships, the document model becomes less appealing.
- it's not possible to say in general which data model leads to simpler application code, it depends on the kinds of relationships that exist between data items.
- no schema means that arbitrary keys and values can be added to a document, and when reading, clients have no guarantees as to what fields the documents may contain.
- document databases are sometimes called schemaless, but that's misleading, a more accurate term is schema-on-read, in contrast with schema-on-write
- the difference between the approaches is particularly noticeable in situations where an application wants to change the format of its data.
- schema changes have a bad reputation of being slow and requiring downtime.
- the schema-on-read approach is advantageous if the items in the collection don't all have the same structure for some reason.
  - there are many different types of objects
  - the structure of the data is determined by external systems over which you have no control and which may change at any time
- a document is usually stored as a single continuous string, encoded as JSON, XML, or a binary variant thereof
- the locality advantage only applies if you need large parts of the document at the same time
- on updates to a document, the entire document usually needs to be rewritten--only modifications that don't change the encoded size of a document can easily be performed in place. it is generally recommended that you keep documents farly small and avoid writes that increase the size of a document
- it's worth pointing out that the idea of grouping related data together for locality is not limited to the document model.
- the same locality properties in relational data model, by allowing the schema to declare that a table's rows should be interleaved with a parent table.
- a hybrid of the relational and document models is a good route for databases to take in the future
- SQL is a declarative query language, while IMS and CODASYL queried the database using imperative code
- many commonly used programming languages are imperative
- an imperative language tells the computer to perform certain operations in a certain order.
- in a declarative query language, like SQL or relational algebra, you just specify the pattern of the data you want
- a declarative query language is attractive because it is typically more concise and easier to work with than an imperative API.
- the sql example doesn't guarantee any particular ordering, and so it doesn't mind if the order changes.
- finally, declarative languages often lend themselves to parallel execution.
- in a web Brower, using declarative css styling is much better than manipulating styles imperatively in javascript
- mapreduce is a programming model for processing large amounts of data in bulk across many machines, popularized by google.
- the map and reduce functions are somewhat restricted in what they are allowed to do, they must be pure functions, which means they only use the data that is passed to them as input, they cannot perform additional database queries, and they must not have any side effects
- there restrictions allow the database to run the functions anywhere, in any order, and rerun them on failure.
- they can parse strings call library functions perform calculations and more.
- mapreduce is a fairly low-level programming model for distributed execution on a cluster of machines.
- a usability problem with mapreduce is that you have to write two carefully coordinated javascript functions which is often harder than writing a single query ( in mongoDB)
- the moral of the story is that a NoSQL system may find itself accidentally reinventing SQL, albeit in disguise.
- the relation model can handle simple cases of many-to-many relationships, but as the connections within your data become more complex, it becomes more natural to start modeling your data as a graph
- typical examples include: social graphs, the web graph, road or rail networks
- pagerank can be used on the web graph to determine the popularity of a web page and thus its ranking in search results.
- graphs are not limited to such homogeneous data: an equally powerful use of graphs is to provide a consistent way of string completely different types of objects in a single datastore.
- two graph models: property graph model (Neo4j, Titan, InfiniteGraph) and triple-store model (Datomic, allegroGraph)
- three declarative query languages for graphs: Cypher, SPARQL, and Datalog
- property graphs
  - each vertex consists of : 
    - a unique identifier
    - a set of outgoing edges
    - a set of incoming edges
    - a collection of properties (key-value pairs)
  - each edge consists of :
    - q unique identifier
    - the vertex at which the edge start(tail vertex)
    - the vertex at which the edge ends (head vertex)
    - a label to describe the kind of relationship between the two vertices
    - a collection of properties (key-value pairs)
- you can think of a graph store as consisting of two relational tables, one for vertices and one for edges
- graph data can be represented in a relational database, so that we can also query it using SQL, however the syntax is very clumsy in comparison to Cyber which is designed for graph
- in a relational database, you usually know in advance which joins you need in your query, in a graph query, you may need to traverse a varible number of edges before you find the vertex you're looking for -- that is , the number of joins is not fixed in advance.
- if the same query can be written in a lines in one query language but requires 29 lines in another, that just shows that differen data models are designed to satisfy different use cases.
- It's important to pick a data model that is suitable for you application.
- 这部分没写graph model最有效的存储和检索方式，这在下一章（就叫存储和检索）
- triple-store model is mostly equivalent to the property graph model, using different words to describe the same idea.主要用来构建语义网络，知识图谱应该就是用这类数据结构
- in a triple-store, all information is stored in the form of very simple three-part statements: (subject, predicate, object)
- the subject of a triple is equivalent to a vertex in a graph, the object is one of two things:
  - a value in a primitive datatype, such as a string or a number. In that case, the predicate and object of the triple are equivalent to the key and value of a property on the subject vertex.
  - another vertex in the graph. in that case, the predicate is an edge in the graph, the subject is the tail vertex, and the object is the head vertex.
- triples can be a good internal data model for applications, even if you have no interest in publishing RDF data on the semantic web . RDF是为语义网络定义的一种数据结构
- 

## Chapter 3 Storage and retrieval

- chapter2 we discussed dta models and query languages--i.e., the format in which you give the database your data, and the mechanism by which you can ask for it again later.
- chapter3 discusses how we can store the data that we're given, and how we can find it again when we're asked for it.
- firstly the chapter talks about storage engines that are used in the kinds of databases that you're probably familiar with: traditional relational databases, and also most so-called NoSQL databases（两类数据模型的存储）.we will examine two families of storage engines: log-structured storage engines, and page-oriented storage engines such as B-trees.
- Many databases internally use a log, which is an append-only data file. Logs are incredibly useful.
  - The word log if often used to refer to application logs, where an application outputs text that describes what's happening. In this book, **log is used in the more general sense: an append-only sequence of records**. It doesn't have to be human-readable; it might be binary and intended only for other programs to read.
- In order to efficiently find the value for a particular key in the database, we need a different data structure: an index.(In this chapter we will look at a range of indexing structures )
- the general idea behind index technologies is to keep some additional metadata on the side, which acts as a signpost and helps you to locate to data you want.
- An index is an additional structure that is derived from the primary data.
- Many databases allow you to add and remove indexes, and this doesn't affect the contents of the database; it only affects the performance of queries.
- Maintaining additional structures incurs overhead, especially on writes, because the index also needs to be updated every time data is written.
- this is an important trade-off in storage systems: well-chosen indexes speed up read queries, but every index slows down writes.
- key-value data is very common. key-value stores are quit similar to the dictionary type that you can find in most programming languages, and which is usually implemented as a hash map (hash table).
- hash map is a commonly used as in-memory data structures, but not the structure used in hard disks. Bitcask (the default storage engine in Riak) uses the approach, which key-value stores as log, and hash map index in memory, when you want to look up a value, use the hasp map to find the offset in the data file, seek to that location, and read the value.
- THE problem is the values of hash map can use more space than there is available memory, since the hash map is kept completely in memory. (hash map for index is viable)
- This approach is for the kind of workload, there are a lot of writes, but there are not too many distinct keys -- you have a large number of writes per key, but it's feasible to keep all keys in memory.
- how do we avoid eventually running out of disk space when stores data in disk by log style? -- break the log into segments of a certain size by closing a segment file when it reaches a certain size and making subsequent writes to a new segment file.
- The segments approach requires compaction, compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.
- Segments are never modified after they have been written, so the merged segment is written to a new file.
- In the has map index, segment file storage style: each segment now has its own in-memory hash table, mapping keys to file offsets. the merging process keeps the number of segments small, so lookups don't need to check many hash maps.
- 就是上述这么简单的思路(log式记录，内存hash map索引，硬盘 segment文件切分+定期merge)，在实现上就有多个问题需要克服：
  - file format: use a binary format that first encodes the length of a string  in bytes, followed by the raw string.
  - deleting records: append a special deletion record to the data file.
  - crash recovery: if the database is restarted, the in-memory hash maps are lost. you can only restore them by rebuilding it. bitcask speeds up recovery  by storing a snapshot of each segment's has map on disk.
  - partially written records: bitcask files include checksums, allowing such corrupted parts of the log to be detected and ignored.
  - concurrency control: a common implementation choice is to have only one writer thread. Data file segments are append-only and otherwise immutable.
- goodness of append-only design:
  - appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drives.
  - concurrency and crash recovery are much simpler if segment files are append-only or immutable.
- limitations of the hash table index:
  - the hash table must fit in memory, so if you have a very large number of keys, you're out of luck.
  - range queries are not efficient.
- 基于(log式记录，内存hash map索引，硬盘 segment文件切分+定期merge)，对segment 文件中的记录按key进行排序（原始的segment文件中，记录的顺序不重要），就是SSTables存储方式。Sorted String Tables, or SSTable for short.
- 
